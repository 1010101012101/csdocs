
https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6

sigmoid
ReLU
tanh
softmax
linear rectification (max{0, x})

---------------

https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9

Learning rate
Epoch : when entire dataset passed back & forth ONCE through neural network 
Batch size : number of training examples per batch
Iterations  :  number of batches needed to complete one epoch

---------------

1. sigmoid
2. hyperbolic tan (tanh)
3. linear rectification (max{0, x})

The former two choices are saturating activation functions because their 
derivatives approach zero as x → ±inf, which can cause gradient-based 
optimization algorithms to converge slowly. The derivative of the 
linear rectification function does not diminish as x → +inf, which helps 
avoid this “vanishing gradient problem” and often results in faster 
training and better performance (Nair & Hinton 2010). Neurons with
this activation function are called rectified linear units (ReLU).
(Shallue and Vanderburg, Exoplanets with deep learning)
