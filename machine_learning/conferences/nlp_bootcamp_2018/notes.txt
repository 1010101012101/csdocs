
--------------

salesforce metaminds einstein.ai


look at gensim, scikit-learn

https://www.hindawi.com/journals/scn/2018/7247095/
https://devblogs.nvidia.com/malware-detection-neural-networks/
http://www.rosecompiler.org/ROSE_ResearchPapers/2009-07-DetectingCodeClonesInBinaryExecutables-ISSTA.pdf

--------
blog.openai.com unsupervised sentiment neuron

blog on ai.google.com - explains auto-reply
https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html

comp linguistics is older approach : teach the grammar

how to handle out of vocabulary words

convert text to numerical form : text rep

feature extraction

deep learning got rid of hand-designed features

distributional representation = cooccurence
linguistic aspect based on co-occurence of words
similar distribution => similar meaning
connotational semantics

distributed representation : factorize = SVD, PCA, tSNE

first build distributional rep
then build distributed rep

using pretrained embedding (word2vec, GloVe) has problems
1) Out of vocab words may be problem
2) transfer learning (new type of text) may not work

Yoon Kim - CNN for txt classification, 2014 (handle missing words)

softmax : converts set of real numbers (z1..zk) into a probab distribution
QUES : is softmax a probab distribution (theoretically)?

No preprocessing (stem, lemma) done on CBOW

how to eval quality of word vectors
1) intrinsic : word vector analogies
2) extrinsic : downstream NLP task

to approx softmax probability, dont use entire vocab
1) use hierarchical softmax
2) sampling techniques - sample some from negative classes

tSNE - low distance retains better than higher dist

tensorboard

perplexity models the noise 

https://distill.pub

Sebastian Ruder   - NLP 
Ricard Socher

word2vec parameter learning explained - X Rong

How to solve word disambiguation problem : same word "bank" in many different contexts in tSNE

FastText classifier from FB


---------------------

ICLR 

Sentiment analysis

1) ordering of words matters : are u good, u are good
2) negation : i do want a car; i dont want a car (do and dont tend to have close wordvec)
3) sentence vectors generated by ops on word vec dont capture syntactic and semantic props

How to embed a sentence to an ID such that
semantically similar sentences map together
word ordering changes the sentence ID

word2vec does not understand negation : do and dont should point in different direction
word2vec does not understand ordering : use Concat instead of Average


Doc2Vec:
Distributed Memory

doc2vec works well if test set is subset of training set
otherwise it requires training at runtime to find embedding for sentences not seen before

Gensim

class imbalance problem in machine learning : occurs during sentence prediction in doc2Vec
since unique sentences are more sparse

skipThoughts

Show and Tell paper
attention network + LSTM = holy grail right now, from explanatory point of view

right now, text models not sophisticated as visual models, because cannot interpret
what a random point in embedding space means


==============

Char2Vec

language models at word/sentence/para/doc level have problems

for limited set of alphabet - except mandarin, japanese

how to verify preprocessing works ?
- do intrinsic audit : compare CDF, post-processed data should have Zipf's law.

construct vector repo from smaller pieces
- morpheme

char embedding - cannot be intrinscally evaluated; can only be evaluated by downstream

tweet2vec

=============

understanding sentence
how to find similar sentences (stackoverflow)
grammar correction (grammarly)
translating sentence to another language

=============

Vision
Text
Video
Audio

In Deep Learning, we learn the filters rather than usng predefined filters

=============

Char CNN :

text CNN filters are not square; they are k x d
1D or temporal convolution

In text, no invariance and composability, unlike in vision

CNN work well for some NLP : sentiment analysis, spam detection, topic categorization

CNNs faster than RNN
Highway network : similar to residual network

CNN useful for classification
What about non-classification - generation ?
 - Sequence labeling (NER)
 - Sequence generation (MT)

Having char2vec can be used to handle out of vocab words

Universal embedding !

Attention network :  paper Show Attend and tell
https://github.com/yunjey/show-attend-and-tell

reversal embedding
context-aware embedding



