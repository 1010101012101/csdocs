
activation func

1. sigmoid
2. hyperbolic tan (tanh)
3. linear rectification (max{0, x})

The former two choices are saturating activation functions because their 
derivatives approach zero as x → ±inf, which can cause gradient-based 
optimization algorithms to converge slowly. The deriva tive of the 
linear rectification function does not diminish as x → +inf, which helps 
avoid this “vanishing gradient problem” and often results in faster 
training and better performance (Nair & Hinton 2010). Neurons with
this activation function are called rectified linear units (ReLU).
(Shallue and Vanderburg, Exoplanets with deep learning)
