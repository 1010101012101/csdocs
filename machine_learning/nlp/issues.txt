
Issues in NLP

how to handle 

1) out of vocab words : use n-grams
2) misspelled words
3) concat words
4) rare words : having common ngrams with frequent words will ensure good embedding
5) word sense disambiguation (WSD)
6) Data sparsity (unable to model all possible strings of words)
7) negation : do vs "don't"
8) ordering of words


=============

Data sparsity

Guthrie, Closer look at skip-gram modeling
http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf

Solutions
1) smoothing
2) POS tags
3) syntactic categories
4) skipgrams

============

word sense disambiguation 
polysemy, homonym

Both CBOW and SG assume unique rep for each word

https://www.quora.com/Can-Word2Vec-be-used-for-Word-Sense-Disambiguation-WSD-If-yes-how

Bartunov, Adaptive skip-gram
https://arxiv.org/pdf/1502.07257.pdf

Qiu, Proximity-Ambiguity Sensitive Skip-gram 

Neelakantan, Multi-sense Skip gram

==============

perplexity

= 2^entropy
= effective number of neighbours

==============

Hyperparameters (in word2vec)

window size

============

Problems with Classes

Class imbalance problem : softmax
Large num of classes : hierarchical softmax

===============

quantization of word vectors to save space
